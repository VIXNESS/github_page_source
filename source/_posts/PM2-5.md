title: PM2.5 é¢„æµ‹
author: Jiang Tao
tags:
  - machine learning
  - linear regression
categories:
  - machine learning
date: 2018-12-24 15:26:00
---
# ä½¿ç”¨Linear Regression å¯¹PM2.5è¿›è¡Œé¢„æµ‹

## æ•°æ®é›†
> [training data](https://github.com/VIXNESS/machine-learning-course/blob/master/pm25_predict/train.csv)
>
> [testing data: samples](https://github.com/VIXNESS/machine-learning-course/blob/master/pm25_predict/test.csv)
>
> [testing data: label](https://github.com/VIXNESS/machine-learning-course/blob/master/pm25_predict/ans.csv)
>
> Training data å’Œ Public testing data çš„ç»„ç»‡å½¢å¼:
> 
> ä¸€å¤©ç”±18è¡Œç»„æˆ,ä¸€è¡Œä¸ºä¸€ä¸ªæŒ‡æ ‡,ä¸€å…±ç”±18ä¸ªæŒ‡æ ‡,ä»ç¬¬4åˆ—å¼€å§‹è®°å½•æ¯ä¸ªæŒ‡æ ‡ä¸€å¤©å†…24å°æ—¶çš„å˜åŒ–æ•°å€¼,æ¯ä¸ªæœˆè¿ç»­è®°å½•å‰20å¤©ä½œä¸ºtraining set,å10å¤©ä½œä¸ºtesting set,ä¸€å…±è®°å½•äº†240ä¸ªå°æ—¶

<!-- more -->

| æ—¥æœŸ   | è§‚æµ‹ç«™ | æŒ‡æ ‡ | 0æ—¶  | ...  | 23æ—¶  |
| ------ | ------ | ---- | ---- | ---- | ---- |
| day 1  |    xxx    |  PM2.5    |      |      |      |
| day 1 | xxx | PM10 |      |      |      |
| day 1 | xxx | SO2 |      |      |      |
| day 1 | xxx | ... |      |      |      |
| day 2 | xxx | PM2.5 | | | |
| day 2 | xxx | PM10 | | | |
| day 2 | xxx | SO2 | | | |
| day 2 | xxx | ... | | | ||

## æ•°æ®å¤„ç†
### å¿…è¦ç±»åº“
```python
import os
import tensorflow as tf
from tensorflow import keras
import csv
import sys
import numpy as np
import matplotlib.pyplot as plt
```

### è½½å…¥æ•°æ®
```python
data = []
for i in range(18):
    data.append([]) # åˆå§‹åŒ–18åˆ—
n_row = 0
with open('train.csv','r',encoding = 'big5') as text: #csvç¼–ç æ˜¯big5
    row = csv.reader(text, delimiter = ",")
    for r in row:
        if n_row != 0:
            for i in range(3,27):
                if r[i] != "NR": #NRä¸ºæœªé™é›¨,å¯¹å…¶è®¾ç½®ä¸ºé™é›¨é‡
                    data[(n_row - 1) % 18].append(float(r[i]))
                else:
                	#è®¾æ–½0é™é›¨ä¸ºä¸€ä¸ªæ¥è¿‘0çš„å°é‡,
                    #è‹¥è®¾ä¸º0,åç»­çš„æ¢¯åº¦è®¡ç®—ä¼šæœ‰é™¤0çš„é£é™©
                    data[(n_row - 1) % 18].append(float(0.0001))
        n_row += 1

```

### é‡æ–°ç»„ç»‡æ•°æ®
> å°†ä¹‹å‰çš„æ•°æ®é‡æ–°ç»„ç»‡,å¯¹æ¯ä¸ªå°æ—¶è¿›è¡Œè¿ç»­æ‹¼æ¥

| Features | 0æ—¶  | ...  | 23æ—¶ | 0æ—¶(æ¬¡æ—¥) | ...  | 23æ—¶ |
| -------- | ---- | ---- | ---- | --------- | ---- | ---- |
| PM2.5    |      |      |      |           |      |      |
| ...      |      |      |      |           |      |      |
| ..       |      |      |      |           |      |      |
| PM 10    |      |      |      |           |      |      | |



```python
x = [] # æ ·æœ¬çŸ©é˜µ
y = [] # å®é™…çš„å€¼
for i in range(12): # 12ä¸ªæœˆ
    for j in range(471): 
    # æ¯è¾“å…¥9ä¸ªå°æ—¶çš„æ•°å€¼,é¢„æµ‹ç¬¬10ä¸ªå°æ—¶çš„PM2.5å€¼,
    # è¿™æ ·è¿ç»­çš„ã€Œ10ä¸ªå°æ—¶ã€æ¯ä¸ªæœˆæœ‰471ä¸ª
        x.append([]) #
        for w in range(18): # éå†18ä¸ªç‰¹å¾
            for t in range(9): # éå†å‰9ä¸ªå°æ—¶
                x[471 * i + j].append(data[w][480 * i + j + t])
        # å°†ç¬¬10ä¸ªå°æ—¶çš„å€¼ä½œä¸ºå®é™…çš„PM2.5çš„å€¼
        y.append(data[9][480 * i + j + 9])
x = np.array(x)
y = np.array(y)

#åœ¨ç¬¬ä¸€åˆ—æ·»ä¸Šä¸€æ¡å…¨ä¸º1çš„åˆ—ä½œä¸ºbias
x = np.concatenate((np.ones((x.shape[0],1)),x),axis = 1) 
w = np.zeros(x.shape[1]) #weight

```
## è®­ç»ƒ
### å®šä¹‰loss function
> ä½¿ç”¨ error square
```python
def lossFunction(target,weight,samples):
    M = target - np.dot(weight,samples.T)
    loss = 0
    for m in M:
        loss += m**2
    return loss
```

### Gradient Descent
> ä½¿ç”¨Adagra å¯¹learning rateè¿›è¡Œæ§åˆ¶

```python
lr = 8 #learning rate è®¾ç½®
pre_grad = np.ones(x.shape[1])# æ¯ä¸ªç‰¹å¾æœ‰ç‹¬ç«‹çš„learning rate
for r in range(10000):
    temp_loss = 0
    for m in range(36):
        for s in range(156):
            L = np.dot(w,x[157 * m + s].T) - y[157 * m + s]
            grad = np.dot(x[157 * m + s].T,L)*(2)
            pre_grad += grad**2
            ada = np.sqrt(pre_grad)
            w = w - lr * grad/ada
        temp_loss += abs(np.dot(w,x[157 * m + 156].T) - y[157 * m + 156])
    print("%.2f" % (r * 100 / 10000),'% loss:',"%.4f" % (temp_loss / 36))

```

> ä¿å­˜ weights
```python
np.save('model.npy',w)
```

## æµ‹è¯•
+ åŠ è½½æµ‹è¯•ç‰¹å¾æ•°æ®é›†(ç•¥)
+ åŠ è½½label

```python
y = []
rr = 0
with open('ans.csv','r',encoding = 'big5') as ans:
    row = csv.reader(ans,delimiter = ',')
    for r in row:
        if rr != 0:
            y.append(float(r[1]))
        rr += 1
y = np.array(y)

```
+ åŠ è½½weights

```python
w = np.load('model.npy')
```

+ æµ‹è¯•

```python
t = np.dot(x,w)
L = t - y
loss = []
sum = 0
for l in L:
    loss.append(abs(l))
    sum += abs(l)
print(sum / len(L))
plt.plot(y,color = "red",label = 'target')
plt.plot(t,color = "blue",label = 'hypothesis')
plt.ylabel('pm 2.5')
plt.show()
```
### ç»“æœ
![ç»“è®º](/images/PM25/model_1.png)
> PM2.5 è¯¯å·® **14.427**  
> ~~å‚æ•°å¤ªå¤šè¿‡æ‹Ÿåˆ~~æ²¡æœ‰è®­ç»ƒå¥½,å¡åœ¨äº†æŸä¸ªåœ°æ–¹äº†,trainingæ—¶å€™çš„lossä¹Ÿå¾ˆé«˜

## å†ä¼˜åŒ–
> åªå–18ä¸ªç‰¹å¾ä¸­çš„NMHCã€NO2ã€O3ã€PM10ã€PM2.5

![model_2](/images/PM25/model_1.png)
> PM2.5 è¯¯å·® **8.987**  
> ä¸€ä¸ªä¸é”™çš„å¼€å¤´,ç»§ç»­ä¼˜åŒ–
> 
> åªè€ƒè™‘PM 10å’ŒPM 2.5

![model_3](/images/PM25/model_3.png)
> PM2.5 è¯¯å·® **6.281**

> è‹¥åœ¨åˆ å‡ç‰¹å¾å‘¢?
> åªè€ƒè™‘ PM2.5 

![model_4](/images/PM25/model_4.png)

> PM2.5 è¯¯å·® **5.406**  
> æˆ‘æœäº†,ä¹‹å‰åšçš„æ—¶å€™æ˜¯ä¼šunderfittingå¯¼è‡´è¯¯å·®åˆ°7.4çš„,è¿™å›å€’å¥½æ›´åŠ ä½äº†

# ä½¿ç”¨DNN å¯¹PM2.5è¿›è¡Œé¢„æµ‹
> ä½¿ç”¨çš„æ˜¯tensorflow + keras
> é¢„å¤‡å·¥ä½œç•¥
## Feature Scaling
> ç”¨äº†ä¸¤ä¸ªä¸åŒçš„Feature Scalingçš„æ–¹æ³•,ç»“æœä¸Šçœ‹å·®åˆ«ä¸å¤§,Standardizationæ›´åŠ å¥½ä¸€ç‚¹
### Standardization

```python
def standardization(dataMatrix):
    if dataMatrix.shape[0] == 0:
        return dataMatrix
    for i in range(dataMatrix.shape[1]):
        sum = 0
        for _x in dataMatrix:
            sum += _x[i]
        mean = sum / dataMatrix.shape[0]
        SD = 0
        for _x in dataMatrix:
            SD += (_x[i] - mean)**2
        SD = np.sqrt(SD / dataMatrix.shape[0])
    
        for _x in dataMatrix:
            _x[i] = (_x[i] - mean) / SD
    return dataMatrix
```
### Mean Normalization

```python
def meanNormalization(dataMatrix):
    if dataMatrix.shape[0] == 0:
        return dataMatrix
    for i in range(dataMatrix.shape[1]):
        sum = 0
        max = 0
        min = 0
        for data in dataMatrix:
            sum += data[i]
            if data[i] > max:
                max = data[i]
            if data[i] < min:
                min = data[i]
        mean = sum / dataMatrix.shape[0]
        if (max - min) != 0:
            for data in dataMatrix:
                data[i] = (data[i] - mean) / (max - min)
    return dataMatrix
```

```python
trainX = standardization(trainX)
testX = standardization(testX)
# trainX = meanNormalization(trainX)
# testX = meanNormalization(testX)
```
## è®­ç»ƒ
> ä½¿ç”¨outputä¸º8çš„ä¸¤å±‚layer,æ¿€æ´»å‡½æ•°æ˜¯ReLU(Sigmoidæ•ˆæœæ›´åŠ å·®)

### å»ºç«‹æ¨¡å‹
```python
model = keras.Sequential([
    keras.layers.Dense(8, activation=tf.nn.relu),
    keras.layers.Dense(8, activation=tf.nn.relu),
    keras.layers.Dense(1)
    ])
model.compile(loss="mse",
        optimizer=tf.train.RMSPropOptimizer(0.001),
        metrics=['mae', 'mse'])
```
### è®­ç»ƒ
> è·‘100ä¸ªepochsåŸºæœ¬ä¸Šæ²¡ä»€ä¹ˆå˜åŒ–äº†
```python
history = model.fit(trainX, 
                    trainY, 
                    batch_size = 64, 
                    epochs = 100, 
                    validation_split = 0.2, 
                    verbose=0, 
                    callbacks=[PrintDot()])
```
> è®­ç»ƒæ—¶å€™çš„loss
![training_loss](/images/PM25/loss.png)

### æµ‹è¯•
> lossæ˜¯**7.47**

![test](/images/PM25/dnn.png)

## å†ä¼˜åŒ–
### ä¸ä½¿ç”¨Feature Scaling
> è®­ç»ƒæ—¶å€™çš„loss
![nonFS](/images/PM25/non_fs.png)

> æµ‹è¯•æ—¶lossæ˜¯**5.170**
![noFSr](/images/PM25/non_fs_rs.png)

### å¢åŠ layers
> å¢åŠ å¤šä¸€å±‚layer  
  è®­ç»ƒæ—¶loss

![3layers](/images/PM25/3layers_loss.png)

> æµ‹è¯•æ—¶loss **7.5**

> æ‰€ä»¥è¯´DNNæœ€ä¼˜èƒ½è¾¾åˆ°5.17, Linear Modelæœ€ä¼˜5.4  
ä¸é”™ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰

# Repo
[VIXNESS/machine-learning-course](https://github.com/VIXNESS/machine-learning-course.git)

