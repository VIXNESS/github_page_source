title: PM2.5 预测
author: Jiang Tao
tags:
  - machine learning
  - linear regression
categories:
  - machine learning
date: 2018-12-24 15:26:00
---
# 使用Linear Regression 对PM2.5进行预测

## 数据集
> [training data](https://github.com/VIXNESS/machine-learning-course/blob/master/pm25_predict/train.csv)
>
> [testing data: samples](https://github.com/VIXNESS/machine-learning-course/blob/master/pm25_predict/test.csv)
>
> [testing data: label](https://github.com/VIXNESS/machine-learning-course/blob/master/pm25_predict/ans.csv)
>
> Training data 和 Public testing data 的组织形式:
> 
> 一天由18行组成,一行为一个指标,一共由18个指标,从第4列开始记录每个指标一天内24小时的变化数值,每个月连续记录前20天作为training set,后10天作为testing set,一共记录了240个小时

<!-- more -->

| 日期   | 观测站 | 指标 | 0时  | ...  | 23时  |
| ------ | ------ | ---- | ---- | ---- | ---- |
| day 1  |    xxx    |  PM2.5    |      |      |      |
| day 1 | xxx | PM10 |      |      |      |
| day 1 | xxx | SO2 |      |      |      |
| day 1 | xxx | ... |      |      |      |
| day 2 | xxx | PM2.5 | | | |
| day 2 | xxx | PM10 | | | |
| day 2 | xxx | SO2 | | | |
| day 2 | xxx | ... | | | ||

## 数据处理
### 必要类库
```python
import os
import tensorflow as tf
from tensorflow import keras
import csv
import sys
import numpy as np
import matplotlib.pyplot as plt
```

### 载入数据
```python
data = []
for i in range(18):
    data.append([]) # 初始化18列
n_row = 0
with open('train.csv','r',encoding = 'big5') as text: #csv编码是big5
    row = csv.reader(text, delimiter = ",")
    for r in row:
        if n_row != 0:
            for i in range(3,27):
                if r[i] != "NR": #NR为未降雨,对其设置为降雨量
                    data[(n_row - 1) % 18].append(float(r[i]))
                else:
                	#设施0降雨为一个接近0的小量,
                    #若设为0,后续的梯度计算会有除0的风险
                    data[(n_row - 1) % 18].append(float(0.0001))
        n_row += 1

```

### 重新组织数据
> 将之前的数据重新组织,对每个小时进行连续拼接

| Features | 0时  | ...  | 23时 | 0时(次日) | ...  | 23时 |
| -------- | ---- | ---- | ---- | --------- | ---- | ---- |
| PM2.5    |      |      |      |           |      |      |
| ...      |      |      |      |           |      |      |
| ..       |      |      |      |           |      |      |
| PM 10    |      |      |      |           |      |      | |



```python
x = [] # 样本矩阵
y = [] # 实际的值
for i in range(12): # 12个月
    for j in range(471): 
    # 每输入9个小时的数值,预测第10个小时的PM2.5值,
    # 这样连续的「10个小时」每个月有471个
        x.append([]) #
        for w in range(18): # 遍历18个特征
            for t in range(9): # 遍历前9个小时
                x[471 * i + j].append(data[w][480 * i + j + t])
        # 将第10个小时的值作为实际的PM2.5的值
        y.append(data[9][480 * i + j + 9])
x = np.array(x)
y = np.array(y)

#在第一列添上一条全为1的列作为bias
x = np.concatenate((np.ones((x.shape[0],1)),x),axis = 1) 
w = np.zeros(x.shape[1]) #weight

```
## 训练
### 定义loss function
> 使用 error square
```python
def lossFunction(target,weight,samples):
    M = target - np.dot(weight,samples.T)
    loss = 0
    for m in M:
        loss += m**2
    return loss
```

### Gradient Descent
> 使用Adagra 对learning rate进行控制

```python
lr = 8 #learning rate 设置
pre_grad = np.ones(x.shape[1])# 每个特征有独立的learning rate
for r in range(10000):
    temp_loss = 0
    for m in range(36):
        for s in range(156):
            L = np.dot(w,x[157 * m + s].T) - y[157 * m + s]
            grad = np.dot(x[157 * m + s].T,L)*(2)
            pre_grad += grad**2
            ada = np.sqrt(pre_grad)
            w = w - lr * grad/ada
        temp_loss += abs(np.dot(w,x[157 * m + 156].T) - y[157 * m + 156])
    print("%.2f" % (r * 100 / 10000),'% loss:',"%.4f" % (temp_loss / 36))

```

> 保存 weights
```python
np.save('model.npy',w)
```

## 测试
+ 加载测试特征数据集(略)
+ 加载label

```python
y = []
rr = 0
with open('ans.csv','r',encoding = 'big5') as ans:
    row = csv.reader(ans,delimiter = ',')
    for r in row:
        if rr != 0:
            y.append(float(r[1]))
        rr += 1
y = np.array(y)

```
+ 加载weights

```python
w = np.load('model.npy')
```

+ 测试

```python
t = np.dot(x,w)
L = t - y
loss = []
sum = 0
for l in L:
    loss.append(abs(l))
    sum += abs(l)
print(sum / len(L))
plt.plot(y,color = "red",label = 'target')
plt.plot(t,color = "blue",label = 'hypothesis')
plt.ylabel('pm 2.5')
plt.show()
```
### 结果
![结论](/images/PM25/model_1.png)
> PM2.5 误差 **14.427**  
> ~~参数太多过拟合~~没有训练好,卡在了某个地方了,training时候的loss也很高

## 再优化
> 只取18个特征中的NMHC、NO2、O3、PM10、PM2.5

![model_2](/images/PM25/model_1.png)
> PM2.5 误差 **8.987**  
> 一个不错的开头,继续优化
> 
> 只考虑PM 10和PM 2.5

![model_3](/images/PM25/model_3.png)
> PM2.5 误差 **6.281**

> 若在删减特征呢?
> 只考虑 PM2.5 

![model_4](/images/PM25/model_4.png)

> PM2.5 误差 **5.406**  
> 我服了,之前做的时候是会underfitting导致误差到7.4的,这回倒好更加低了

# 使用DNN 对PM2.5进行预测
> 使用的是tensorflow + keras
> 预备工作略
## Feature Scaling
> 用了两个不同的Feature Scaling的方法,结果上看差别不大,Standardization更加好一点
### Standardization

```python
def standardization(dataMatrix):
    if dataMatrix.shape[0] == 0:
        return dataMatrix
    for i in range(dataMatrix.shape[1]):
        sum = 0
        for _x in dataMatrix:
            sum += _x[i]
        mean = sum / dataMatrix.shape[0]
        SD = 0
        for _x in dataMatrix:
            SD += (_x[i] - mean)**2
        SD = np.sqrt(SD / dataMatrix.shape[0])
    
        for _x in dataMatrix:
            _x[i] = (_x[i] - mean) / SD
    return dataMatrix
```
### Mean Normalization

```python
def meanNormalization(dataMatrix):
    if dataMatrix.shape[0] == 0:
        return dataMatrix
    for i in range(dataMatrix.shape[1]):
        sum = 0
        max = 0
        min = 0
        for data in dataMatrix:
            sum += data[i]
            if data[i] > max:
                max = data[i]
            if data[i] < min:
                min = data[i]
        mean = sum / dataMatrix.shape[0]
        if (max - min) != 0:
            for data in dataMatrix:
                data[i] = (data[i] - mean) / (max - min)
    return dataMatrix
```

```python
trainX = standardization(trainX)
testX = standardization(testX)
# trainX = meanNormalization(trainX)
# testX = meanNormalization(testX)
```
## 训练
> 使用output为8的两层layer,激活函数是ReLU(Sigmoid效果更加差)

### 建立模型
```python
model = keras.Sequential([
    keras.layers.Dense(8, activation=tf.nn.relu),
    keras.layers.Dense(8, activation=tf.nn.relu),
    keras.layers.Dense(1)
    ])
model.compile(loss="mse",
        optimizer=tf.train.RMSPropOptimizer(0.001),
        metrics=['mae', 'mse'])
```
### 训练
> 跑100个epochs基本上没什么变化了
```python
history = model.fit(trainX, 
                    trainY, 
                    batch_size = 64, 
                    epochs = 100, 
                    validation_split = 0.2, 
                    verbose=0, 
                    callbacks=[PrintDot()])
```
> 训练时候的loss
![training_loss](/images/PM25/loss.png)

### 测试
> loss是**7.47**

![test](/images/PM25/dnn.png)

## 再优化
### 不使用Feature Scaling
> 训练时候的loss
![nonFS](/images/PM25/non_fs.png)

> 测试时loss是**5.170**
![noFSr](/images/PM25/non_fs_rs.png)

### 增加layers
> 增加多一层layer  
  训练时loss

![3layers](/images/PM25/3layers_loss.png)

> 测试时loss **7.5**

> 所以说DNN最优能达到5.17, Linear Model最优5.4  
不错🎉🎉🎉🎉🎉🎉

# Repo
[VIXNESS/machine-learning-course](https://github.com/VIXNESS/machine-learning-course.git)

